main actions in this paper was to take small ReLU networks trained on synthetic data with sparse input feature - to investigate how and when models represent more features than they have dimensions 

they also wanted to show that superposition can occur in small "traditional" ML systems emulating that behavior may occur in larger systems 

superposition: when models represent more features than they have dimensions -- when features are sparse, superposition allows compression beyond what a linear model can do, at the cost of "interference" that requires non-linear methods to resolve
